---
title: "Final Assignment"
author: "Group 4"
date: "2025-03-16"
output: html_document
---
https://github.com/racheldolson/Biol-432-Final-Assignment 

Group members: Matteo Ienzi (20270101), Rachel Dolson (20339605), Christina Meier (20213829), Sara Gama (20292893), Isabella Infusino (20218982), Winson Huang (20235342)

## Dataset Description

In this data set, we have many samples from benign (n = 357) and malignant (n = 212) breast cancer tumors with feature information such as radius, texture, perimeter, smoothness and more. There are 10 parameters in the data set, each parameter is measured based on their mean, standard error and the maximum value. We will determine the major predictors of the two types of tumors by creating a regression tree. The results from this tree will be used in the RDA to make a more accurate analysis. From this information, it's plausible to train a ML model to predict whether samples given these features are benign or malignant. We choose to do an RDA because it is a compromise between LDA and QDA, therefore getting the best model possible. It will be successful if it can accurately predict if the samples are cancerous or not. Additionally, we also aim to address which physical characteristics correlate most to a sample either being benign or cancerous. This will be done by running a PCA, and running a biplot to visualize directionality of feature importance.

Learning, U. M. (2016, September 25). Breast cancer wisconsin (diagnostic) data set. Kaggle. https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data 

## Questions we are interested in answering

(1) What are the major features classifying benign and malignant tumors? 
(2) Can we predict whether a sample is benign or cancerous? 
(3) Which features are most correlated with a benign/malignant tumor? 

## Loading and checking data

```{r}
Data <- read.csv("data.csv")
```

```{r}
str(Data)
head(data)
dim(data)
```

## Loading packages

```{r, include=FALSE}
library(BiocManager)
library(Biostrings)
library(reshape2)
library(rentrez)
library(annotate)
library(ape)
library(ggtree)
library(ggplot2)
library(dplyr)
library(tree)
library(randomForest)
library(gbm)
library(vegan)
library(rpart)
library(rpart.plot)
library(caret)
```

## Mutating data

```{r}
data<-Data %>% 
  mutate(diagnosis = as.factor(diagnosis)) #Changing the diagnosis column from a character to a factor
```

```{r}
diagData<-data %>% 
  select(-c("id","X")) #Removing the id and X columns as they are not needed for our analysis
```

## Making regression tree

```{r}
DiagTree<-tree(diagnosis ~ ., data=diagData, method = "class")
plot(DiagTree)
text(DiagTree, cex=0.6, adj=0)
```

```{r}
DiagTreeRpart<-rpart(diagnosis ~ ., data=diagData, method = "class")
rpart.plot(DiagTreeRpart, type = 2, extra = 104, tweak = 1.2, fallen.leaves=TRUE)
```

```{r}
predictions<-predict(DiagTreeRpart, diagData, type="class")
confusionMatrix(predictions, diagData$diagnosis)
```

## Variable importance

```{r}
importance <- as.data.frame(DiagTreeRpart$variable.importance)
print(importance)

par(mar = c(7, 4, 2, 2))
plot(DiagTreeRpart$variable.importance, xlab=" ", 
    ylab="Importance", xaxt = "n", pch=20)
axis(1, at=1:16, labels=row.names(importance), las=2, cex.axis = 0.7)
```

## RDA

```{r}
top_predictors <- row.names(importance)
TopPred <- diagData[, top_predictors, drop = FALSE]
Diag <- as.factor(diagData$diagnosis)

rda_model <- rda(TopPred ~ Diag, data = diagData)
summary(rda_model)
plot(rda_model, scaling = 2)
```

## Seeing the influence parameters have on diagnosis

```{r}
set.seed(123)
DiagBoost<-gbm(diagnosis ~ ., data=diagData, distribution="gaussian", n.trees=25, interaction.depth=2, cv.folds=12)
summary(DiagBoost)
```

## Machine Learning Analysis

All model training, evaluation and plotting were done in 'ml_models.R'.
Due to these steps being computationally intensive, we ran this script externally and noted the inputs and outputs.

- Inputs: Breast cancer dataset with selected predictors
- Outputs:
  - rda_model, lda_model, rf_model, svm_model - trained models
  - top_features_barplot - Random Forest importance features
  - conf_matrix_results_long - confusion matrix of all the models into barplots
  - roc_curve_comparison - ROC curves
  - auc_results - AUC scores


## PCA

```{r}
sum(is.na(Data))
```

```{r}
Data$diagnosis <- factor(Data$diagnosis, levels = c("B", "M"))

# Drop diagnosis column from the dataset for PCA 
X <- Data %>%
  select(-diagnosis, -id)   
```

```{r}
cor_matrix <- cor(X)
cor_matrix
```

```{r}
# Standardize the features 
X_scaled <- scale(X)
```

```{r}
pca_1 <- princomp(X_scaled, cor = T)
```

```{r}
# Summarize PCA to view the explained variance
summary(pca_1)
```

```{r}
names(pca_1)
```

```{r}
eigenvectors <- pca_1$loadings
```

```{r}
principal_components <- pca_1$scale
```

```{r}
combined_dat <- cbind(cancerdat, pca_1$scores)

head(combined_dat)
```

```{r}
ggplot(combined_dat, aes(x = Comp.1, y = Comp.2, color = diagnosis))+
  geom_point()+
  theme_minimal()+
  labs(x = "PC1", y= "PC2")
```

```{r}
ggplot(combined_dat, aes(x = Comp.3, y = Comp.4, color = diagnosis))+
  geom_point()+
  theme_minimal()+
  labs(x = "PC3", y= "PC4")
```

```{r}
loadings(pca_1)
```

```{r}
summary(pca_1)
```

```{r}
# Scree plot - variance explained by each principal component
screeplot(pca_1, main = "Scree Plot", col = "blue", lwd = 2)
```
